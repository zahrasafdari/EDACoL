{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDIs5bIzhlj2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate, LSTM, Permute, Attention, Lambda, Dot, Activation, Concatenate, Layer, Flatten, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Embedding\n",
        "\n",
        "# Load data\n",
        "daily_data = np.load('sea-ice-prediction-main/sea-ice-prediction-main/climate-change-ai-workshop/data/dailyt30_features.npy', allow_pickle=True)\n",
        "monthly_data = np.load('sea-ice-prediction-main/sea-ice-prediction-main/climate-change-ai-workshop/data/monthly_features.npy', allow_pickle=True)\n",
        "daily_target = np.load('sea-ice-prediction-main/sea-ice-prediction-main/climate-change-ai-workshop/data/dailyt30_target.npy', allow_pickle=True)\n",
        "monthly_target = np.load('sea-ice-prediction-main/sea-ice-prediction-main/climate-change-ai-workshop/data/monthly_target.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "daily_data = daily_data[:-7, :, :]\n",
        "monthly_data = monthly_data[:-7, :, :]\n",
        "monthly_target = monthly_target[:-7]\n",
        "\n",
        "data = np.concatenate((daily_data, monthly_data), axis=1)\n",
        "lag = 1\n",
        "data = data[:-lag, :, :]\n",
        "monthly_target = monthly_target[lag:]\n",
        "\n",
        "LEN_DATA = len(data)\n",
        "NUM_TRAIN = LEN_DATA - (12 * 20)\n",
        "NUM_VALID = LEN_DATA - NUM_TRAIN\n",
        "\n",
        "x_train = data[0:NUM_TRAIN]\n",
        "x_valid = data[NUM_TRAIN:]\n",
        "\n",
        "y_train = monthly_target[:NUM_TRAIN]\n",
        "y_valid = monthly_target[NUM_TRAIN:]\n",
        "\n",
        "# Normalize features\n",
        "scaler_f = StandardScaler()\n",
        "x_train = scaler_f.fit_transform(x_train.reshape(-1, x_train.shape[2]))\n",
        "x_valid = scaler_f.transform(x_valid.reshape(-1, x_valid.shape[2]))\n",
        "\n",
        "scaler_l = StandardScaler()\n",
        "y_train = scaler_l.fit_transform(y_train.reshape(-1, 1))\n",
        "y_valid = scaler_l.transform(y_valid.reshape(-1, 1))\n",
        "\n",
        "# Reshape features\n",
        "def reshape_features(dataset, timesteps=1):\n",
        "    return dataset.reshape((int(dataset.shape[0] / timesteps)), timesteps, dataset.shape[1])\n",
        "\n",
        "timesteps = 31\n",
        "x_train = reshape_features(x_train, timesteps)\n",
        "x_valid = reshape_features(x_valid, timesteps)\n",
        "\n",
        "# Split the reshaped features into two separate inputs for the model\n",
        "x_train1 = x_train[:, :30, :]\n",
        "x_train2 = x_train[:, 30:31, :]\n",
        "\n",
        "x_valid1 = x_valid[:, :30, :]\n",
        "x_valid2 = x_valid[:, 30:31, :]\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, Conv1D,\n",
        "                                     MultiHeadAttention, LayerNormalization,\n",
        "                                     concatenate, Permute, Add, GlobalAveragePooling1D)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_advanced_model():\n",
        "    # Input layers\n",
        "    model1_input = Input(shape=(30, x_train1.shape[-1]))\n",
        "    model2_input = Input(shape=(1, x_train2.shape[-1]))\n",
        "\n",
        "    # Model 1\n",
        "    model1 = Permute((2, 1))(model1_input)\n",
        "    model1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(model1)\n",
        "    model1 = MultiHeadAttention(num_heads=8, key_dim=64)(model1, model1)\n",
        "    model1 = Dropout(0.4)(model1)\n",
        "    model1 = LSTM(256, return_sequences=True)(model1)\n",
        "    model1 = Dropout(0.4)(model1)\n",
        "    model1 = LSTM(128, return_sequences=True)(model1)\n",
        "    model1 = Dropout(0.4)(model1)\n",
        "    model1 = GlobalAveragePooling1D()(model1)\n",
        "    model1_output = Dense(64, activation='relu')(model1)\n",
        "    model1_output = Dense(1)(model1_output)\n",
        "\n",
        "    # Model 2\n",
        "    model2 = Permute((2, 1))(model2_input)\n",
        "    model2 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(model2)\n",
        "    model2 = MultiHeadAttention(num_heads=8, key_dim=64)(model2, model2)\n",
        "    model2 = Dropout(0.4)(model2)\n",
        "    model2 = LSTM(256, return_sequences=True)(model2)\n",
        "    model2 = Dropout(0.4)(model2)\n",
        "    model2 = LSTM(128, return_sequences=True)(model2)\n",
        "    model2 = Dropout(0.4)(model2)\n",
        "    model2 = GlobalAveragePooling1D()(model2)\n",
        "    model2_output = Dense(64, activation='relu')(model2)\n",
        "    model2_output = Dense(1)(model2_output)\n",
        "\n",
        "    # Concatenate models\n",
        "    ensemble = concatenate([model1_output, model2_output])\n",
        "    merged_model = Dense(64, activation='relu')(ensemble)\n",
        "    merged_model = Dense(1)(merged_model)\n",
        "\n",
        "    # Build and compile model\n",
        "    model = Model(inputs=[model1_input, model2_input], outputs=merged_model)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compile and Train the Model\n",
        "model = build_advanced_model()\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, mode='min', min_delta=0.001),\n",
        "    tf.keras.callbacks.ModelCheckpoint('./advanced_model.h5.keras', monitor='val_loss', save_best_only=True, mode='min')\n",
        "]\n",
        "\n",
        "# Train model\n",
        "history = model.fit([x_train1, x_train2], y_train, epochs=500, batch_size=64,\n",
        "                    validation_split=0.3, callbacks=callbacks)\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_weights('./advanced_model.h5.keras')\n",
        "\n",
        "# Predictions\n",
        "train_pred = model.predict([x_train1, x_train2])\n",
        "test_pred = model.predict([x_valid1, x_valid2])\n",
        "\n",
        "# Evaluate and visualize results\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot predictions vs actuals\n",
        "plt.scatter(y_train, train_pred)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=4)\n",
        "plt.xlabel('Observed')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(y_valid, valid_pred)\n",
        "plt.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], 'k--', lw=4)\n",
        "plt.xlabel('Observed')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Calculate RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Calculate NRMSE\n",
        "def nrmse(y_true, y_pred):\n",
        "    return rmse(y_true, y_pred) / (np.max(y_true) - np.min(y_true))\n",
        "\n",
        "# Calculate R-squared\n",
        "def r_squared(y_true, y_pred):\n",
        "    return r2_score(y_true, y_pred)\n",
        "\n",
        "# Compute metrics on training and validation data\n",
        "train_rmse = rmse(y_train, train_pred)\n",
        "valid_rmse = rmse(y_valid, test_pred)\n",
        "\n",
        "train_nrmse = nrmse(y_train, train_pred)\n",
        "valid_nrmse = nrmse(y_valid, test_pred)\n",
        "\n",
        "train_r2 = r_squared(y_train, train_pred)\n",
        "valid_r2 = r_squared(y_valid, test_pred)\n",
        "\n",
        "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Validation RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Training NRMSE: {train_nrmse:.4f}\")\n",
        "print(f\"Validation NRMSE: {test_nrmse:.4f}\")\n",
        "print(f\"Training R-squared: {train_r2:.4f}\")\n",
        "print(f\"Validation R-squared: {test_r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "F5rabhHphpcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
